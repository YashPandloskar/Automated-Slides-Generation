{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pandl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pandl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pandl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing image descriptions...\n",
      "Image folder modified - regenerating descriptions\n",
      "Generating new image descriptions...\n",
      "Processing: image_page10_img1.jpeg\n",
      "Processing: image_page10_img2.jpeg\n",
      "Processing: image_page11_img1.jpeg\n",
      "Processing: image_page11_img2.jpeg\n",
      "Processing: image_page1_img1.jpeg\n",
      "Processing: image_page1_img2.jpeg\n",
      "Processing: image_page1_img3.jpeg\n",
      "Processing: image_page5_img1.jpeg\n",
      "Processing: image_page7_img1.jpeg\n",
      "Processing: image_page8_img3.jpeg\n",
      "Processing: image_page9_img3.jpeg\n",
      "Image descriptions cached to C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\image_descriptions_cache.json\n",
      "Reading presentation content...\n",
      "Creating presentation with images...\n",
      "Selected image 'image_page11_img2.jpeg' for slide 'Introduction'\n",
      "  Section type: introduction, Score: 4.37\n",
      "Note: Graph/chart image 'image_page11_img2.jpeg' might not be ideal for 'Introduction', but allowing it.\n",
      "Selected image 'image_page11_img2.jpeg' for slide 'Introduction'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page11_img2.jpeg\n",
      "Successfully added image 'image_page11_img2.jpeg' to slide 'Introduction'\n",
      "Selected image 'image_page10_img1.jpeg' for slide 'Key Topics and'\n",
      "  Section type: key_topics, Score: 1.53\n",
      "Note: Graph/chart image 'image_page10_img1.jpeg' might not be ideal for 'Key Topics and', but allowing it.\n",
      "Selected image 'image_page10_img1.jpeg' for slide 'Key Topics and'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page10_img1.jpeg\n",
      "Successfully added image 'image_page10_img1.jpeg' to slide 'Key Topics and'\n",
      "Selected methodology diagram 'image_page5_img1.jpeg' for slide 'Methods and Approaches'\n",
      "Note: Graph/chart image 'image_page5_img1.jpeg' might not be ideal for 'Methods and Approaches', but allowing it.\n",
      "Selected image 'image_page5_img1.jpeg' for slide 'Methods and Approaches'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page5_img1.jpeg\n",
      "Successfully added image 'image_page5_img1.jpeg' to slide 'Methods and Approaches'\n",
      "Selected image 'image_page9_img3.jpeg' for slide 'Findings'\n",
      "  Section type: results, Score: 2.28\n",
      "Selected image 'image_page9_img3.jpeg' for slide 'Findings'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page9_img3.jpeg\n",
      "Successfully added image 'image_page9_img3.jpeg' to slide 'Findings'\n",
      "Selected methodology diagram 'image_page7_img1.jpeg' for slide 'Important Statistics Data'\n",
      "Note: Architecture/diagram image 'image_page7_img1.jpeg' would be better in a methodology section, but allowing it.\n",
      "Selected image 'image_page7_img1.jpeg' for slide 'Important Statistics Data'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page7_img1.jpeg\n",
      "Successfully added image 'image_page7_img1.jpeg' to slide 'Important Statistics Data'\n",
      "Selected methodology diagram 'image_page11_img1.jpeg' for slide 'Applications and Use'\n",
      "Note: Architecture/diagram image 'image_page11_img1.jpeg' would be better in a methodology section, but allowing it.\n",
      "Selected image 'image_page11_img1.jpeg' for slide 'Applications and Use'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page11_img1.jpeg\n",
      "Successfully added image 'image_page11_img1.jpeg' to slide 'Applications and Use'\n",
      "Selected methodology diagram 'image_page1_img2.jpeg' for slide 'Challenges and Limitations'\n",
      "Note: Architecture/diagram image 'image_page1_img2.jpeg' would be better in a methodology section, but allowing it.\n",
      "Selected image 'image_page1_img2.jpeg' for slide 'Challenges and Limitations'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page1_img2.jpeg\n",
      "Successfully added image 'image_page1_img2.jpeg' to slide 'Challenges and Limitations'\n",
      "Selected image 'image_page8_img3.jpeg' for slide 'Future Scope Recommendations'\n",
      "  Section type: methodology, Score: 1.21\n",
      "Note: Graph/chart image 'image_page8_img3.jpeg' might not be ideal for 'Future Scope Recommendations', but allowing it.\n",
      "Selected image 'image_page8_img3.jpeg' for slide 'Future Scope Recommendations'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page8_img3.jpeg\n",
      "Successfully added image 'image_page8_img3.jpeg' to slide 'Future Scope Recommendations'\n",
      "Selected image 'image_page10_img2.jpeg' for slide 'Conclusion'\n",
      "  Section type: methodology, Score: 0.90\n",
      "Note: Graph/chart image 'image_page10_img2.jpeg' might not be ideal for 'Conclusion', but allowing it.\n",
      "Selected image 'image_page10_img2.jpeg' for slide 'Conclusion'\n",
      "Adding image from path: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\\image_page10_img2.jpeg\n",
      "Successfully added image 'image_page10_img2.jpeg' to slide 'Conclusion'\n",
      "✅ Presentation saved at: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Generated_Presentation_new.pptx\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "from pptx.util import Pt, Inches\n",
    "from pptx.enum.text import MSO_AUTO_SIZE, PP_ALIGN\n",
    "import re\n",
    "import os\n",
    "import ollama\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "\n",
    "# Download NLTK resources if not already available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Define file paths\n",
    "content_file_path = r\"C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\presentation_content.txt\"\n",
    "pptx_output_path = r\"C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Generated_Presentation_new.pptx\"\n",
    "image_folder = r\"C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Extracted\"\n",
    "\n",
    "# Constants\n",
    "MAX_CHARS_PER_SLIDE = 500  # Text limit per slide for readability\n",
    "BODY_FONT_SIZE = Pt(18)    # Standardized body font size\n",
    "IMAGE_WIDTH = Inches(4)    # Standard image width for presentations\n",
    "MAX_IMAGES_PER_SLIDE = 1   # Maximum number of images per slide\n",
    "\n",
    "# Sections to exclude entirely from the presentation\n",
    "SECTIONS_TO_EXCLUDE = [\n",
    "    \"Definition\", \"Key Part\", \"Key Parts\", \"Definitions\", \n",
    "    \"Definitions and key\", \"Definitions and Key\", \n",
    "    \"Definition and key\", \"Definition and Key\"\n",
    "]\n",
    "\n",
    "# Title renaming mappings\n",
    "TITLE_RENAMES = {\n",
    "    \"Key topics and\": \"Key Topics\",\n",
    "    \"Important Statistics and\": \"Important Statistics\",\n",
    "    \"Future Scope and\": \"Future Scope\"\n",
    "}\n",
    "\n",
    "def describe_images_in_folder(folder_path):\n",
    "    \"\"\"Get detailed descriptions of images with improved prompting for technical diagrams.\"\"\"\n",
    "    image_descriptions = {}\n",
    "\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        if image_name.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".webp\")):\n",
    "            print(f\"Processing: {image_name}\")\n",
    "\n",
    "            try:\n",
    "                # Enhanced prompt specifically for technical presentations\n",
    "                response = ollama.chat(\n",
    "                    model=\"llava\",\n",
    "                    messages=[{\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"\"\"Please analyze this image in detail for a technical presentation:\n",
    "1. CONTENT TYPE: Is this a diagram/flowchart, graph/chart, architectural design, conceptual illustration, or something else?\n",
    "2. SECTION CATEGORIZATION: Would this image best fit in (Introduction/Background, Methodology, Results/Findings, or Key Topics)?\n",
    "3. TECHNICAL ELEMENTS: What specific technical elements, components, or data are shown?\n",
    "4. VISIBLE TEXT: List any important text visible in the image.\n",
    "5. TECHNICAL DOMAIN: What scientific or technical domain does this relate to (ML/AI, medical, engineering, etc.)?\"\"\",\n",
    "                        \"images\": [image_path]\n",
    "                    }],\n",
    "                )\n",
    "\n",
    "                description = response.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "                image_descriptions[image_name] = description\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {image_name}: {e}\")\n",
    "                image_descriptions[image_name] = \"Error processing image\"\n",
    "\n",
    "    return image_descriptions\n",
    "\n",
    "def categorize_image_by_type(description):\n",
    "    \"\"\"Categorize images into types based on their descriptions.\"\"\"\n",
    "    categories = {\n",
    "        \"methodology\": [\"architecture\", \"diagram\", \"workflow\", \"process\", \"flowchart\", \n",
    "                       \"pipeline\", \"framework\", \"system design\", \"steps\", \"algorithm\", \n",
    "                       \"method\", \"approach\", \"procedure\", \"implementation\", \"structure\",\n",
    "                       \"design\", \"model architecture\", \"model structure\", \"component\"],\n",
    "        \"results\": [\"graph\", \"plot\", \"chart\", \"result\", \"performance\", \"accuracy\", \n",
    "                   \"metric\", \"evaluation\", \"comparison\", \"outcome\", \"data visualization\"],\n",
    "        \"introduction\": [\"concept\", \"overview\", \"introduction\", \"background\"],\n",
    "        \"key_topics\": [\"key\", \"topics\", \"important\", \"main\", \"highlight\"]\n",
    "    }\n",
    "    \n",
    "    # Score each category\n",
    "    scores = {category: 0 for category in categories}\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in description.lower():\n",
    "                scores[category] += 1\n",
    "                \n",
    "                # Give higher weight to exact matches for methodology diagrams\n",
    "                if category == \"methodology\" and keyword in [\"diagram\", \"architecture\", \"workflow\", \"framework\"]:\n",
    "                    scores[category] += 2\n",
    "    \n",
    "    # Get the highest scoring category\n",
    "    best_category = max(scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Only return if score is above threshold\n",
    "    if best_category[1] > 0:\n",
    "        return best_category[0]\n",
    "    else:\n",
    "        return \"general\"  # Default category\n",
    "\n",
    "def extract_keywords(text, importance_multiplier=1):\n",
    "    \"\"\"Extract weighted keywords from text, handling domain-specific terminology better.\"\"\"\n",
    "    # Clean text\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "    \n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Extended stopwords - very comprehensive\n",
    "    extended_stopwords = {\n",
    "        'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what', 'when', \n",
    "        'where', 'how', 'why', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', \n",
    "        'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "        'having', 'do', 'does', 'did', 'doing', 'to', 'from', 'by', 'for', 'with', 'about', \n",
    "        'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    "        'below', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "        'there', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', \n",
    "        'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "        'can', 'will', 'just', 'should', 'now', 'would', 'could', 'may', 'might'\n",
    "    }\n",
    "    \n",
    "    # Define domain-specific keywords with higher weights\n",
    "    domain_keywords = {\n",
    "        # Medical/health terms\n",
    "        'cancer': 3, 'tumor': 3, 'sarcoma': 3, 'tissue': 2, 'patient': 2, 'clinical': 2,\n",
    "        'medical': 2, 'health': 2, 'diagnosis': 3, 'treatment': 2, 'therapy': 2,\n",
    "        'prognosis': 3, 'survival': 3, 'pathology': 3, 'oncology': 3,\n",
    "        \n",
    "        # Machine learning/data science terms\n",
    "        'algorithm': 3, 'model': 2, 'classifier': 3, 'classification': 3, 'prediction': 2,\n",
    "        'accuracy': 2, 'precision': 2, 'recall': 2, 'feature': 2, 'training': 2,\n",
    "        'validation': 2, 'testing': 2, 'dataset': 2, 'data': 2, 'machine': 2,\n",
    "        'learning': 2, 'neural': 3, 'network': 2, 'deep': 2, 'ensemble': 3,\n",
    "        \n",
    "        # Research/academic terms\n",
    "        'research': 2, 'study': 2, 'analysis': 2, 'results': 2, 'method': 2,\n",
    "        'methodology': 2, 'conclusion': 2, 'findings': 2, 'literature': 2,\n",
    "        'review': 2, 'paper': 2, 'publication': 2, 'journal': 2, 'statistical': 2\n",
    "    }\n",
    "    \n",
    "    # Create keyword dictionary with weights\n",
    "    keywords = {}\n",
    "    for word in words:\n",
    "        if word not in extended_stopwords and len(word) > 2:\n",
    "            # Check if it's a domain keyword (with higher weight)\n",
    "            if word in domain_keywords:\n",
    "                weight = domain_keywords[word] * importance_multiplier\n",
    "            else:\n",
    "                weight = 1 * importance_multiplier\n",
    "                \n",
    "            keywords[word] = keywords.get(word, 0) + weight\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def calculate_image_relevance(slide_title, slide_content, image_description):\n",
    "    \"\"\"Calculate how relevant an image is to a slide using multiple techniques.\"\"\"\n",
    "    # Extract keywords with different weights (title is more important)\n",
    "    title_keywords = extract_keywords(slide_title, importance_multiplier=3)\n",
    "    content_keywords = extract_keywords(slide_content, importance_multiplier=1)\n",
    "    image_keywords = extract_keywords(image_description, importance_multiplier=1)\n",
    "    \n",
    "    # Combine slide keywords\n",
    "    slide_keywords = {}\n",
    "    for word, weight in title_keywords.items():\n",
    "        slide_keywords[word] = weight\n",
    "    for word, weight in content_keywords.items():\n",
    "        if word in slide_keywords:\n",
    "            slide_keywords[word] += weight\n",
    "        else:\n",
    "            slide_keywords[word] = weight\n",
    "    \n",
    "    # Calculate relevance score\n",
    "    relevance_score = 0\n",
    "    matched_keywords = []\n",
    "    \n",
    "    for word, slide_weight in slide_keywords.items():\n",
    "        if word in image_keywords:\n",
    "            # The score is the product of the weights from both sources\n",
    "            keyword_score = slide_weight * image_keywords[word]\n",
    "            relevance_score += keyword_score\n",
    "            matched_keywords.append((word, keyword_score))\n",
    "    \n",
    "    # Normalize score based on number of keywords (to prevent bias toward longer texts)\n",
    "    normalization_factor = max(len(slide_keywords), 1)\n",
    "    normalized_score = relevance_score / normalization_factor\n",
    "    \n",
    "    # Get top matched keywords for debugging\n",
    "    matched_keywords.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_matches = matched_keywords[:5] if matched_keywords else []\n",
    "    \n",
    "    return normalized_score, top_matches\n",
    "\n",
    "def find_best_image_for_slide(slide_title, slide_content, image_descriptions, used_images):\n",
    "    \"\"\"Find the most relevant image for a slide with improved section-based matching.\"\"\"\n",
    "    # Skip if no content\n",
    "    if not slide_title.strip() or not slide_content.strip():\n",
    "        return None\n",
    "    \n",
    "    # Determine section type from slide title - with more flexible matching\n",
    "    section_type = \"general\"\n",
    "    section_title_lower = slide_title.lower()\n",
    "    \n",
    "    # More flexible methodology section detection\n",
    "    if any(term in section_title_lower for term in [\"methodology\", \"method\", \"approach\", \"process\", \"procedure\", \"implementation\"]):\n",
    "        section_type = \"methodology\"\n",
    "    elif any(term in section_title_lower for term in [\"result\", \"finding\", \"outcome\", \"performance\"]):\n",
    "        section_type = \"results\"\n",
    "    elif any(term in section_title_lower for term in [\"introduction\", \"background\", \"overview\"]):\n",
    "        section_type = \"introduction\"\n",
    "    elif any(term in section_title_lower for term in [\"key topic\", \"main point\", \"highlight\"]):\n",
    "        section_type = \"key_topics\"\n",
    "    \n",
    "    # Also check content for methodology terms if section_type is still general\n",
    "    if section_type == \"general\":\n",
    "        if any(term in slide_content.lower() for term in [\"methodology\", \"method\", \"approach\", \"process\", \"procedure\"]):\n",
    "            section_type = \"methodology\"\n",
    "    \n",
    "    # Filter out used images\n",
    "    available_images = {img: desc for img, desc in image_descriptions.items() if img not in used_images}\n",
    "    if not available_images:\n",
    "        return None\n",
    "    \n",
    "    # Track best match\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    best_methodology_match = None\n",
    "    best_methodology_score = 0\n",
    "    \n",
    "    # First prioritize matching by section type\n",
    "    for img_name, img_desc in available_images.items():\n",
    "        # Categorize the image\n",
    "        img_category = categorize_image_by_type(img_desc)\n",
    "        \n",
    "        # Calculate base relevance using existing method\n",
    "        base_score, top_matches = calculate_image_relevance(slide_title, slide_content, img_desc)\n",
    "        \n",
    "        # Boost score if category matches section type\n",
    "        final_score = base_score\n",
    "        if img_category == section_type:\n",
    "            final_score *= 2.0  # Double the score for matching categories\n",
    "            \n",
    "        # Special case for diagrams in methodology - with higher boost\n",
    "        if section_type == \"methodology\" and any(kw in img_desc.lower() for kw in \n",
    "                                              [\"diagram\", \"architecture\", \"workflow\", \"framework\", \"process\"]):\n",
    "            final_score *= 2.0  # Increase the boost for diagrams in methodology\n",
    "            \n",
    "            # Track best methodology diagram separately\n",
    "            if final_score > best_methodology_score:\n",
    "                best_methodology_score = final_score\n",
    "                best_methodology_match = img_name\n",
    "            \n",
    "        # Update best match if this is better\n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            best_match = img_name\n",
    "    \n",
    "    # Use a lower threshold for methodology sections to ensure diagrams get used\n",
    "    threshold = 0.5\n",
    "    if section_type == \"methodology\":\n",
    "        threshold = 0.3  # Lower threshold for methodology sections\n",
    "        \n",
    "        # If we found a good methodology diagram, use it\n",
    "        if best_methodology_match and best_methodology_score > 0.2:\n",
    "            print(f\"Selected methodology diagram '{best_methodology_match}' for slide '{slide_title}'\")\n",
    "            return best_methodology_match\n",
    "    \n",
    "    if best_score >= threshold:\n",
    "        print(f\"Selected image '{best_match}' for slide '{slide_title}'\")\n",
    "        print(f\"  Section type: {section_type}, Score: {best_score:.2f}\")\n",
    "        return best_match\n",
    "    else:\n",
    "        print(f\"No sufficiently relevant image found for slide '{slide_title}'\")\n",
    "        return None\n",
    "    \n",
    "def validate_image_assignment(section_title, image_name, image_description):\n",
    "    \"\"\"Validate that the image is appropriate for the section.\"\"\"\n",
    "    section_lower = section_title.lower()\n",
    "    \n",
    "    # More flexible matching for methodology sections\n",
    "    is_methodology_section = any(term in section_lower for term in \n",
    "                               [\"methodology\", \"method\", \"approach\", \"process\", \"procedure\", \"implementation\"])\n",
    "    \n",
    "    # Architecture diagrams should go in methodology sections, but with more flexibility\n",
    "    if (\"architecture\" in image_description.lower() or \n",
    "        \"diagram\" in image_description.lower() or \n",
    "        \"workflow\" in image_description.lower() or\n",
    "        \"framework\" in image_description.lower() or\n",
    "        \"process\" in image_description.lower()):\n",
    "        \n",
    "        # Allow diagrams in methodology sections with more flexible matching\n",
    "        if not is_methodology_section:\n",
    "            print(f\"Note: Architecture/diagram image '{image_name}' would be better in a methodology section, but allowing it.\")\n",
    "            # Still allow the assignment, just with a warning\n",
    "            return True\n",
    "    \n",
    "    # Results visualizations should preferably go in results sections, but allow flexibility\n",
    "    if (\"graph\" in image_description.lower() or \n",
    "        \"chart\" in image_description.lower() or \n",
    "        \"plot\" in image_description.lower()):\n",
    "        \n",
    "        # Just log a warning but don't block assignment\n",
    "        if \"result\" not in section_lower and \"finding\" not in section_lower:\n",
    "            print(f\"Note: Graph/chart image '{image_name}' might not be ideal for '{section_title}', but allowing it.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text without relying on NLTK.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Simple tokenization using regex\n",
    "    import re\n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    \n",
    "    # Basic stopwords list (you can expand this)\n",
    "    basic_stopwords = {'the', 'a', 'an', 'and', 'is', 'are', 'in', 'of', 'to', 'for', 'on', 'with', 'at', 'by', 'from', 'this', 'that', 'it', 'as'}\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    tokens = [word for word in tokens if word not in basic_stopwords]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def rename_title(title):\n",
    "    \"\"\"Renames specific titles according to predefined mapping.\"\"\"\n",
    "    for old_title, new_title in TITLE_RENAMES.items():\n",
    "        if old_title in title:\n",
    "            return title.replace(old_title, new_title)\n",
    "    return title\n",
    "\n",
    "def read_presentation_content(file_path):\n",
    "    \"\"\"Reads structured content from a text file and returns a dictionary.\"\"\"\n",
    "    presentation_content = {}\n",
    "    current_section = None\n",
    "    content_lines = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Ignore empty lines\n",
    "                if line.endswith(\":\"):  # Identify section headers\n",
    "                    if current_section:  # Save the previous section\n",
    "                        # Check if the current section should be excluded based on exact match or substring\n",
    "                        should_exclude = False\n",
    "                        for exclude in SECTIONS_TO_EXCLUDE:\n",
    "                            if exclude.lower() in current_section.lower():\n",
    "                                should_exclude = True\n",
    "                                break\n",
    "                        \n",
    "                        if not should_exclude:\n",
    "                            # Apply title renaming before adding to presentation content\n",
    "                            renamed_section = rename_title(current_section)\n",
    "                            presentation_content[renamed_section] = \"\\n\".join(content_lines)\n",
    "                    \n",
    "                    current_section = line[:-1]  # Remove the \":\" at the end\n",
    "                    content_lines = []  # Reset content list\n",
    "                else:\n",
    "                    content_lines.append(line)\n",
    "\n",
    "        # Save the last section\n",
    "        if current_section:\n",
    "            should_exclude = False\n",
    "            for exclude in SECTIONS_TO_EXCLUDE:\n",
    "                if exclude.lower() in current_section.lower():\n",
    "                    should_exclude = True\n",
    "                    break\n",
    "            \n",
    "            if not should_exclude:\n",
    "                # Apply title renaming before adding to presentation content\n",
    "                renamed_section = rename_title(current_section)\n",
    "                presentation_content[renamed_section] = \"\\n\".join(content_lines)\n",
    "\n",
    "    return presentation_content\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Removes ampersand signs and other unwanted characters from title.\"\"\"\n",
    "    # First apply any required renaming\n",
    "    title = rename_title(title)\n",
    "    \n",
    "    # Remove ampersand signs\n",
    "    cleaned_title = title.replace(\"&\", \"and\")\n",
    "    \n",
    "    # Remove other potential special characters if needed\n",
    "    cleaned_title = re.sub(r'[^\\w\\s\\-.,:]', '', cleaned_title)\n",
    "    \n",
    "    return cleaned_title.strip()\n",
    "\n",
    "def generate_short_title(long_title):\n",
    "    \"\"\"Creates a meaningful short title by extracting key terms.\"\"\"\n",
    "    # First clean the title\n",
    "    clean_long_title = clean_title(long_title)\n",
    "    \n",
    "    common_keywords = [\"introduction\", \"background\", \"conclusion\", \"references\", \"summary\", \"methodology\", \"results\", \"discussion\", \"findings\"]\n",
    "    \n",
    "    words = clean_long_title.split()\n",
    "    \n",
    "    # If title has a common keyword, return it\n",
    "    for word in words:\n",
    "        if word.lower() in common_keywords:\n",
    "            return word.capitalize()\n",
    "    \n",
    "    # Otherwise, take the first 3 words and make a short phrase\n",
    "    return \" \".join(words[:3])\n",
    "\n",
    "def split_text(text, max_chars):\n",
    "    \"\"\"Splits text into chunks that fit within the character limit per slide.\"\"\"\n",
    "    # Split by paragraphs first\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    slides_content = []\n",
    "    current_slide_text = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # If adding this paragraph exceeds the limit\n",
    "        if len(current_slide_text) + len(para) + 1 > max_chars and current_slide_text:\n",
    "            slides_content.append(current_slide_text.strip())\n",
    "            current_slide_text = para\n",
    "        else:\n",
    "            if current_slide_text:\n",
    "                current_slide_text += \"\\n\" + para\n",
    "            else:\n",
    "                current_slide_text = para\n",
    "    \n",
    "    # Don't forget the last slide\n",
    "    if current_slide_text.strip():\n",
    "        slides_content.append(current_slide_text.strip())\n",
    "        \n",
    "    return slides_content\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using common end-of-sentence markers.\"\"\"\n",
    "    # Define sentence delimiters\n",
    "    sentence_ends = ['. ', '! ', '? ', '.\\n', '!\\n', '?\\n']\n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        current_sentence += text[i]\n",
    "        \n",
    "        # Check if we're at a sentence boundary\n",
    "        for end in sentence_ends:\n",
    "            if i + len(end) <= len(text) and text[i:i+len(end)] == end:\n",
    "                sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "                i += len(end) - 1  # -1 because we'll increment i again at the end of the loop\n",
    "                break\n",
    "                \n",
    "        i += 1\n",
    "    \n",
    "    # Add any remaining text\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(current_sentence.strip())\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "def filter_content_by_section(section_title, content):\n",
    "    \"\"\"Filter content to ensure it's appropriate for the section title.\"\"\"\n",
    "    # Convert section title to lowercase for easier matching\n",
    "    section_lower = section_title.lower()\n",
    "    \n",
    "    # Define patterns that should be excluded from specific sections\n",
    "    exclude_patterns = {\n",
    "        \"introduction\": [\n",
    "            r\"key find(ing|ings)\",\n",
    "            r\"result(s)?( show)?\",\n",
    "            r\"conclusion\",\n",
    "            r\"we conclude\"\n",
    "        ],\n",
    "        \"background\": [\n",
    "            r\"key find(ing|ings)\",\n",
    "            r\"result(s)?( show)?\",\n",
    "            r\"conclusion\",\n",
    "            r\"we conclude\"\n",
    "        ],\n",
    "        \"methodology\": [\n",
    "            r\"key find(ing|ings)\",\n",
    "            r\"conclusion\",\n",
    "            r\"we conclude\"\n",
    "        ],\n",
    "        \"conclusion\": [\n",
    "            r\"introduce\",\n",
    "            r\"background\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Additional patterns to exclude definitions and key parts from any section\n",
    "    definition_patterns = [\n",
    "        r\"is defined as\",\n",
    "        r\"refers to\",\n",
    "        r\"can be defined as\",\n",
    "        r\"is a term used to describe\",\n",
    "        r\"means\",\n",
    "        r\"the definition of\",\n",
    "        r\"key (concept|part|element|component)\"\n",
    "    ]\n",
    "    \n",
    "    # Add definition patterns to all sections\n",
    "    for section_key in exclude_patterns.keys():\n",
    "        exclude_patterns[section_key].extend(definition_patterns)\n",
    "    \n",
    "    # Determine which section type this is\n",
    "    section_type = None\n",
    "    for key in exclude_patterns.keys():\n",
    "        if key in section_lower:\n",
    "            section_type = key\n",
    "            break\n",
    "    \n",
    "    # If no specific section type identified, still filter for definitions\n",
    "    if not section_type:\n",
    "        section_type = \"general\"\n",
    "        exclude_patterns[\"general\"] = definition_patterns\n",
    "    \n",
    "    # Split into sentences to filter at the sentence level\n",
    "    sentences = split_into_sentences(content)\n",
    "    filtered_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        exclude = False\n",
    "        for pattern in exclude_patterns[section_type]:\n",
    "            if re.search(pattern, sentence, re.IGNORECASE):\n",
    "                exclude = True\n",
    "                break\n",
    "        \n",
    "        if not exclude:\n",
    "            filtered_sentences.append(sentence)\n",
    "    \n",
    "    # Rebuild the text\n",
    "    return \" \".join(filtered_sentences)\n",
    "\n",
    "def clean_content(text):\n",
    "    \"\"\"Remove meta-commentary, unnecessary phrases, asterisks, definitions, and key parts from the content.\"\"\"\n",
    "    # List of phrases to remove\n",
    "    phrases_to_remove = [\n",
    "        \"The provided text appears to be\",\n",
    "        \"This text appears to be\",\n",
    "        \"The text you provided appears to be\",\n",
    "        \"It appears that you've provided\",\n",
    "        \"Here are some key points that can be identified from the text\",\n",
    "        \"Here's a brief summary\",\n",
    "        \"Here's a breakdown of the content\",\n",
    "        \"Here are the key findings\",\n",
    "        \"I'll provide a summary and highlight the main points\",\n",
    "        \"Overall, the text provides\",\n",
    "        \"Overall, this paper presents\",\n",
    "        \"Here's a summary of the main points\",\n",
    "        \"Here are some key points extracted from the text\",\n",
    "        \"Some possible limitations or areas for further investigation include\",\n",
    "        \"Some key contributions of the proposed research include\",\n",
    "        \"Some potential improvements or follow-up questions based on this text could include\"\n",
    "    ]\n",
    "    \n",
    "    # Remove phrases\n",
    "    clean_text = text\n",
    "    for phrase in phrases_to_remove:\n",
    "        clean_text = clean_text.replace(phrase, \"\")\n",
    "    \n",
    "    # Remove sentences containing meta-commentary\n",
    "    sentences = split_into_sentences(clean_text)\n",
    "    filtered_sentences = []\n",
    "    \n",
    "    meta_patterns = [\n",
    "        r\"The (text|document|paper|provided text) (appears to be|is|discusses|highlights|covers|reviews|presents)\",\n",
    "        r\"It (appears|seems) (that|like)\",\n",
    "        r\"The (author|authors) (provides|provide|highlights|highlight|discusses|discuss|aims|aim|mentions|mention)\",\n",
    "        r\"This (appears|seems) to be\",\n",
    "        r\"(From|Based on) (the|this) (text|content|document)\"\n",
    "    ]\n",
    "    \n",
    "    # Additional patterns to filter out definitions and key parts\n",
    "    definition_key_patterns = [\n",
    "        r\"is defined as\",\n",
    "        r\"refers to\",\n",
    "        r\"can be defined as\",\n",
    "        r\"is a term used to describe\",\n",
    "        r\"means\",\n",
    "        r\"the definition of\",\n",
    "        r\"key (concept|part|element|component)\"\n",
    "    ]\n",
    "    \n",
    "    meta_patterns.extend(definition_key_patterns)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        is_meta = False\n",
    "        for pattern in meta_patterns:\n",
    "            if re.search(pattern, sentence, re.IGNORECASE):\n",
    "                is_meta = True\n",
    "                break\n",
    "        \n",
    "        if not is_meta:\n",
    "            filtered_sentences.append(sentence)\n",
    "    \n",
    "    # Rebuild the text\n",
    "    clean_text = \" \".join(filtered_sentences)\n",
    "    \n",
    "    # Remove double spaces and clean up\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    \n",
    "    # Remove ALL asterisks (both standalone and those used as bullet points)\n",
    "    clean_text = clean_text.replace(\"*\", \"\")\n",
    "    \n",
    "    # Remove any remaining references to \"the text\" or \"the document\"\n",
    "    clean_text = re.sub(r'\\b(the text|the document)\\b', '', clean_text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def prepare_content_for_slides(section_title, text):\n",
    "    \"\"\"Clean the content and convert it to separate paragraphs, filtering by section type.\"\"\"\n",
    "    # First, clean the content and remove asterisks\n",
    "    text = clean_content(text)\n",
    "    \n",
    "    # Filter content based on section type\n",
    "    text = filter_content_by_section(section_title, text)\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    # Group sentences into paragraphs based on context\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # Skip very short sentences or fragments\n",
    "        if len(sentence.split()) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Add to current paragraph\n",
    "        if current_paragraph:\n",
    "            current_paragraph += \" \" + sentence\n",
    "        else:\n",
    "            current_paragraph = sentence\n",
    "            \n",
    "        # If sentence is conceptually complete (based on length or ending), start a new paragraph\n",
    "        if len(current_paragraph.split()) > 20 or sentence.endswith('.'):\n",
    "            paragraphs.append(current_paragraph)\n",
    "            current_paragraph = \"\"\n",
    "    \n",
    "    # Add any remaining content\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(current_paragraph)\n",
    "    \n",
    "    # Return as separate lines\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "def add_slide(prs, title_text, content_text, image_descriptions, used_images):\n",
    "    \"\"\"Adds one or more slides with content and relevant images, ensuring consistent formatting.\"\"\"\n",
    "    # Apply title renaming\n",
    "    title_text = rename_title(title_text)\n",
    "    \n",
    "    # Check if this slide should be excluded based on its title\n",
    "    should_exclude = False\n",
    "    for exclude in SECTIONS_TO_EXCLUDE:\n",
    "        if exclude.lower() in title_text.lower():\n",
    "            print(f\"Skipping slide with title: '{title_text}' (matches exclusion criteria)\")\n",
    "            return\n",
    "    \n",
    "    # Prepare content for slides (clean and format)\n",
    "    formatted_content = prepare_content_for_slides(title_text, content_text)\n",
    "    \n",
    "    # If no content remains after filtering, skip this slide\n",
    "    if not formatted_content.strip():\n",
    "        print(f\"Warning: No appropriate content for slide '{title_text}' after filtering\")\n",
    "        return\n",
    "    \n",
    "    # Split content into multiple slides if needed\n",
    "    slides_data = split_text(formatted_content, MAX_CHARS_PER_SLIDE)\n",
    "    \n",
    "    # Clean title and generate short title\n",
    "    cleaned_title_text = clean_title(title_text)\n",
    "    short_title = generate_short_title(cleaned_title_text)\n",
    "    \n",
    "    # Check if the short title contains any exclusion terms\n",
    "    for exclude in SECTIONS_TO_EXCLUDE:\n",
    "        if exclude.lower() in short_title.lower():\n",
    "            print(f\"Skipping slide with short title: '{short_title}' (matches exclusion criteria)\")\n",
    "            return\n",
    "    \n",
    "    if not slides_data:  # Handle empty content case\n",
    "        return\n",
    "    \n",
    "    # Try to find a single best image for all slides in this section\n",
    "    best_image = find_best_image_for_slide(short_title, formatted_content, image_descriptions, used_images)\n",
    "    if best_image:\n",
    "        # Validate the image assignment\n",
    "        if validate_image_assignment(short_title, best_image, image_descriptions[best_image]):\n",
    "            used_images.add(best_image)\n",
    "            print(f\"Selected image '{best_image}' for slide '{short_title}'\")\n",
    "        else:\n",
    "            best_image = None  # Don't use this image if validation fails\n",
    "        \n",
    "    for i, slide_text in enumerate(slides_data):\n",
    "        # Create new slide\n",
    "        slide_layout = prs.slide_layouts[1]  # Title & Content Layout\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        \n",
    "        # Set the title with proper formatting\n",
    "        title = slide.shapes.title\n",
    "        slide_title = short_title if i == 0 else f\"{short_title} (contd.)\"\n",
    "        title.text = slide_title\n",
    "        \n",
    "        # Format title text - ensure it's visible\n",
    "        title_text_frame = title.text_frame\n",
    "        title_text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "        for paragraph in title_text_frame.paragraphs:\n",
    "            paragraph.font.size = Pt(32)\n",
    "            paragraph.font.bold = True\n",
    "            paragraph.alignment = PP_ALIGN.CENTER\n",
    "        \n",
    "        # Get content placeholder\n",
    "        content_shape = slide.placeholders[1]  # Content area (index 1)\n",
    "        \n",
    "        if not content_shape:\n",
    "            print(f\"Warning: No content placeholder found for slide {i+1}\")\n",
    "            continue\n",
    "            \n",
    "        # Determine if this slide should have an image\n",
    "        has_image = best_image and i == 0\n",
    "        \n",
    "        # Get slide dimensions\n",
    "        slide_width = prs.slide_width\n",
    "        slide_height = prs.slide_height\n",
    "        \n",
    "        # *** IMPORTANT FIX: Properly layout content and image to prevent overlap ***\n",
    "        if has_image:\n",
    "            # Store original dimensions\n",
    "            original_width = content_shape.width\n",
    "            original_left = content_shape.left\n",
    "            original_top = content_shape.top\n",
    "            original_height = content_shape.height\n",
    "            \n",
    "            # Calculate new dimensions for text area (left side)\n",
    "            # Reduce width to 55% of slide width to make space for image\n",
    "            new_text_width = int(slide_width * 0.55)\n",
    "            \n",
    "            # Reposition and resize content shape BEFORE adding text\n",
    "            content_shape.left = original_left\n",
    "            content_shape.top = original_top\n",
    "            content_shape.width = new_text_width\n",
    "            # Keep the original height\n",
    "            \n",
    "            # Calculate image dimensions and position (right side)\n",
    "            img_width = int(slide_width * 0.38)  # 38% of slide width\n",
    "            img_left = content_shape.left + content_shape.width + Inches(0.2)  # Add spacing between text and image\n",
    "            img_top = content_shape.top + Inches(0.1)  # Slight offset from content top\n",
    "        \n",
    "        # Process text content with proper sizing\n",
    "        text_frame = content_shape.text_frame\n",
    "        text_frame.clear()  # Clear any existing text\n",
    "        \n",
    "        # Control text frame properties\n",
    "        text_frame.word_wrap = True\n",
    "        text_frame.auto_size = MSO_AUTO_SIZE.SHAPE_TO_FIT_TEXT\n",
    "        \n",
    "        # Format content as properly formatted paragraphs\n",
    "        lines = slide_text.strip().split(\"\\n\")\n",
    "        \n",
    "        # Only proceed if we have content\n",
    "        if not lines or not lines[0].strip():\n",
    "            continue\n",
    "        \n",
    "        # First paragraph\n",
    "        first_line = lines[0].strip()\n",
    "        p = text_frame.paragraphs[0]\n",
    "        p.text = first_line\n",
    "        p.level = 0  # Set indentation level\n",
    "            \n",
    "        # Apply consistent formatting\n",
    "        p.font.size = BODY_FONT_SIZE\n",
    "        p.font.bold = False\n",
    "        p.space_after = Pt(10)\n",
    "        \n",
    "        # Remaining paragraphs - with consistent formatting\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            p = text_frame.add_paragraph()\n",
    "            p.text = line\n",
    "            p.level = 0  # Set indentation level\n",
    "                \n",
    "            # Apply consistent formatting to all paragraphs\n",
    "            p.font.size = BODY_FONT_SIZE\n",
    "            p.font.bold = False\n",
    "            p.space_after = Pt(10)\n",
    "        \n",
    "        # Now add the image if we have one, after text content is set\n",
    "        if has_image:\n",
    "            image_path = os.path.join(image_folder, best_image)\n",
    "            # Add image to the slide if the file exists\n",
    "            if os.path.exists(image_path):\n",
    "                try:\n",
    "                    # CORRECTED CODE: Ensure absolute path and proper error handling\n",
    "                    abs_image_path = os.path.abspath(image_path)\n",
    "                    print(f\"Adding image from path: {abs_image_path}\")\n",
    "                    \n",
    "                    # Add the image with the calculated dimensions to prevent overlap\n",
    "                    picture = slide.shapes.add_picture(\n",
    "                        abs_image_path,\n",
    "                        left=img_left,\n",
    "                        top=img_top,\n",
    "                        width=img_width\n",
    "                    )\n",
    "                    \n",
    "                    # Optional: Check if image is too tall and resize proportionally if needed\n",
    "                    if picture.height > (slide_height * 0.7):\n",
    "                        height_ratio = (slide_height * 0.7) / picture.height\n",
    "                        picture.height = int(picture.height * height_ratio)\n",
    "                        picture.width = int(picture.width * height_ratio)\n",
    "                    \n",
    "                    print(f\"Successfully added image '{best_image}' to slide '{slide_title}'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding image '{best_image}' to slide: {str(e)}\")\n",
    "                    # Try alternative approach if first method fails\n",
    "                    try:\n",
    "                        picture = slide.shapes.add_picture(\n",
    "                            abs_image_path,\n",
    "                            left=Inches(5),  # Fixed position as fallback\n",
    "                            top=Inches(2),\n",
    "                            width=Inches(4)\n",
    "                        )\n",
    "                        print(f\"Added image using fallback method\")\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Second attempt failed: {str(e2)}\")\n",
    "\n",
    "def create_presentation(presentation_content, image_descriptions):\n",
    "    \"\"\"Generates a PowerPoint presentation with consistent formatting and relevant images.\"\"\"\n",
    "    \n",
    "    prs = Presentation()\n",
    "    \n",
    "    # Keep track of used images to avoid duplicates\n",
    "    used_images = set()\n",
    "\n",
    "    # Add a Title Slide\n",
    "    title_slide_layout = prs.slide_layouts[0]  # Title Slide Layout\n",
    "    slide = prs.slides.add_slide(title_slide_layout)\n",
    "    title = slide.shapes.title\n",
    "    subtitle = slide.placeholders[1]  # Placeholder for subtitle\n",
    "\n",
    "    title.text = \"Ensemble Methods for High-Performance Classification\"\n",
    "    subtitle.text = \"A Study on Adult Soft Tissue Sarcomas\"\n",
    "    \n",
    "    # Format title slide text\n",
    "    title.text_frame.paragraphs[0].font.size = Pt(44)\n",
    "    subtitle.text_frame.paragraphs[0].font.size = Pt(28)\n",
    "    \n",
    "    # Try to find a suitable image for the title slide\n",
    "    title_image = find_best_image_for_slide(\n",
    "        \"Ensemble Methods for High-Performance Classification\", \n",
    "        \"\",  # Adding slide content\n",
    "        image_descriptions,\n",
    "        used_images\n",
    "    )\n",
    "    \n",
    "    if title_image:\n",
    "        used_images.add(title_image)\n",
    "        image_path = os.path.join(image_folder, title_image)\n",
    "        if os.path.exists(image_path):\n",
    "            # Position title slide image at the bottom right\n",
    "            img_width = Inches(3)\n",
    "            img_left = prs.slide_width - img_width - Inches(0.5)  # Right side with margin\n",
    "            img_top = prs.slide_height - Inches(2.5)  # Bottom with margin\n",
    "            \n",
    "            try:\n",
    "                slide.shapes.add_picture(image_path, img_left, img_top, width=img_width)\n",
    "                print(f\"Added image '{title_image}' to title slide\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding image to title slide: {e}\")\n",
    "\n",
    "    # Add Content Slides\n",
    "    for section, content in presentation_content.items():\n",
    "        # Apply any title renaming first\n",
    "        section = rename_title(section)\n",
    "        \n",
    "        # Extra check to exclude any sections with \"definitions and key\" in the title\n",
    "        should_exclude = False\n",
    "        for exclude in SECTIONS_TO_EXCLUDE:\n",
    "            if exclude.lower() in section.lower():\n",
    "                print(f\"Skipping section: '{section}' (matches exclusion criteria)\")\n",
    "                should_exclude = True\n",
    "                break\n",
    "        \n",
    "        if not should_exclude:\n",
    "            add_slide(prs, section, content, image_descriptions, used_images)\n",
    "\n",
    "    # Save PowerPoint File\n",
    "    try:\n",
    "        prs.save(pptx_output_path)\n",
    "        print(f\"✅ Presentation saved at: {pptx_output_path}\")\n",
    "    except PermissionError:\n",
    "        # Try with a different filename if permission error\n",
    "        alt_path = pptx_output_path.replace(\".pptx\", \"_new.pptx\")\n",
    "        prs.save(alt_path)\n",
    "        print(f\"✅ Presentation saved at: {alt_path}\")\n",
    "\n",
    "def prepare_image_content(folder_path, content_file_path, source_pdf_path=None, force_refresh=False):\n",
    "    \"\"\"\n",
    "    Prepare image descriptions, using cache if appropriate.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing extracted images\n",
    "        content_file_path: Path to content file\n",
    "        source_pdf_path: Path to source PDF (used for cache invalidation)\n",
    "        force_refresh: Whether to force regeneration of descriptions\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of image descriptions\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import hashlib\n",
    "    import time\n",
    "    \n",
    "    # Define a cache file path based on the content file path\n",
    "    cache_dir = os.path.dirname(content_file_path)\n",
    "    cache_file = os.path.join(cache_dir, \"image_descriptions_cache.json\")\n",
    "    \n",
    "    # Get folder modification time to detect new images\n",
    "    try:\n",
    "        folder_mtime = os.path.getmtime(folder_path)\n",
    "    except:\n",
    "        folder_mtime = time.time()  # Default to current time if folder doesn't exist\n",
    "    \n",
    "    # Calculate hash of source PDF if available (to detect new source documents)\n",
    "    source_hash = None\n",
    "    if source_pdf_path and os.path.exists(source_pdf_path):\n",
    "        try:\n",
    "            with open(source_pdf_path, 'rb') as f:\n",
    "                content = f.read(8192)  # Read first 8KB to create hash\n",
    "                source_hash = hashlib.md5(content).hexdigest()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not hash source PDF: {e}\")\n",
    "    \n",
    "    # Try to load cached descriptions if they exist and we're not forcing refresh\n",
    "    if os.path.exists(cache_file) and not force_refresh:\n",
    "        try:\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                cache_data = json.load(f)\n",
    "                \n",
    "                # Check cache validity - needs same source document and folder hasn't been modified\n",
    "                cache_is_valid = True\n",
    "                \n",
    "                # Check cache version\n",
    "                if cache_data.get('version', 0) != 1:\n",
    "                    cache_is_valid = False\n",
    "                    print(\"Cache version mismatch - regenerating\")\n",
    "                \n",
    "                # Check if source document changed\n",
    "                if source_hash and cache_data.get('source_hash') != source_hash:\n",
    "                    cache_is_valid = False\n",
    "                    print(\"Source document changed - regenerating descriptions\")\n",
    "                \n",
    "                # Check if image folder was modified after cache creation\n",
    "                if cache_data.get('folder_mtime', 0) < folder_mtime:\n",
    "                    cache_is_valid = False\n",
    "                    print(\"Image folder modified - regenerating descriptions\")\n",
    "                \n",
    "                # Return descriptions if cache is valid\n",
    "                if cache_is_valid:\n",
    "                    print(f\"Loading cached image descriptions from {cache_file}\")\n",
    "                    return cache_data.get('descriptions', {})\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cache: {e}. Generating new descriptions.\")\n",
    "    \n",
    "    # If no cache, invalid cache, or error, generate new descriptions\n",
    "    print(\"Generating new image descriptions...\")\n",
    "    image_descriptions = describe_images_in_folder(folder_path)\n",
    "    \n",
    "    # Save the descriptions to cache for future use\n",
    "    try:\n",
    "        cache_data = {\n",
    "            'version': 1,  # Cache version for future compatibility\n",
    "            'timestamp': time.time(),\n",
    "            'folder_mtime': folder_mtime,\n",
    "            'source_hash': source_hash,\n",
    "            'descriptions': image_descriptions\n",
    "        }\n",
    "        \n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cache_data, f, indent=2)\n",
    "            print(f\"Image descriptions cached to {cache_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cache: {e}\")\n",
    "    \n",
    "    return image_descriptions\n",
    "\n",
    "# Update the main function to use the improved prepare_image_content function\n",
    "def main(source_pdf_path=None, force_refresh_cache=False):\n",
    "    \"\"\"\n",
    "    Main execution flow with improved caching\n",
    "    \n",
    "    Args:\n",
    "        source_pdf_path: Path to source PDF document (optional)\n",
    "        force_refresh_cache: Whether to force regeneration of image descriptions\n",
    "    \"\"\"\n",
    "    # Get image descriptions (with improved caching)\n",
    "    print(\"Preparing image descriptions...\")\n",
    "    image_descriptions = prepare_image_content(\n",
    "        image_folder, \n",
    "        content_file_path,\n",
    "        source_pdf_path=source_pdf_path,\n",
    "        force_refresh=force_refresh_cache\n",
    "    )\n",
    "    \n",
    "    # Read presentation content\n",
    "    print(\"Reading presentation content...\")\n",
    "    presentation_content = read_presentation_content(content_file_path)\n",
    "    \n",
    "    # Generate PowerPoint with images\n",
    "    print(\"Creating presentation with images...\")\n",
    "    create_presentation(presentation_content, image_descriptions)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
