{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pandl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Sarcoma.pdf\n",
      "Error handling collection: In Chroma v0.6.0, list_collections only returns collection names. Use Client.get_collection(doc_collection_864eccd4ee_1745473552) to access name. See https://docs.trychroma.com/deployment/migration for more information.\n",
      "Attempting fallback with collection name: doc_collection_864eccd4ee_1745474897\n",
      "Processing section: Title & Introduction\n",
      "  ROUGE scores for 'Title & Introduction':\n",
      "    ROUGE-1 F1: 0.2302\n",
      "    ROUGE-2 F1: 0.0928\n",
      "    ROUGE-L F1: 0.1178\n",
      "Processing section: Key Topics & Sections\n",
      "  ROUGE scores for 'Key Topics & Sections':\n",
      "    ROUGE-1 F1: 0.2619\n",
      "    ROUGE-2 F1: 0.0930\n",
      "    ROUGE-L F1: 0.1187\n",
      "Processing section: Definitions & Key Terms\n",
      "  ROUGE scores for 'Definitions & Key Terms':\n",
      "    ROUGE-1 F1: 0.2032\n",
      "    ROUGE-2 F1: 0.0404\n",
      "    ROUGE-L F1: 0.0991\n",
      "Processing section: Methods & Approaches\n",
      "  ROUGE scores for 'Methods & Approaches':\n",
      "    ROUGE-1 F1: 0.1960\n",
      "    ROUGE-2 F1: 0.0676\n",
      "    ROUGE-L F1: 0.0986\n",
      "Processing section: Findings & Results\n",
      "  ROUGE scores for 'Findings & Results':\n",
      "    ROUGE-1 F1: 0.2389\n",
      "    ROUGE-2 F1: 0.0595\n",
      "    ROUGE-L F1: 0.1188\n",
      "Processing section: Important Statistics & Data\n",
      "  ROUGE scores for 'Important Statistics & Data':\n",
      "    ROUGE-1 F1: 0.2686\n",
      "    ROUGE-2 F1: 0.1210\n",
      "    ROUGE-L F1: 0.1195\n",
      "Processing section: Applications & Use Cases\n",
      "  ROUGE scores for 'Applications & Use Cases':\n",
      "    ROUGE-1 F1: 0.2413\n",
      "    ROUGE-2 F1: 0.1088\n",
      "    ROUGE-L F1: 0.1152\n",
      "Processing section: Challenges & Limitations\n",
      "  ROUGE scores for 'Challenges & Limitations':\n",
      "    ROUGE-1 F1: 0.2446\n",
      "    ROUGE-2 F1: 0.1192\n",
      "    ROUGE-L F1: 0.1190\n",
      "Processing section: Future Scope & Recommendations\n",
      "  ROUGE scores for 'Future Scope & Recommendations':\n",
      "    ROUGE-1 F1: 0.2793\n",
      "    ROUGE-2 F1: 0.1385\n",
      "    ROUGE-L F1: 0.1409\n",
      "Processing section: Conclusion & Summary\n",
      "  ROUGE scores for 'Conclusion & Summary':\n",
      "    ROUGE-1 F1: 0.1870\n",
      "    ROUGE-2 F1: 0.0835\n",
      "    ROUGE-L F1: 0.0980\n",
      "\n",
      "===== ROUGE Score Summary =====\n",
      "Average ROUGE-1 F1: 0.2351\n",
      "Average ROUGE-2 F1: 0.0924\n",
      "Average ROUGE-L F1: 0.1146\n",
      "===============================\n",
      "\n",
      "ðŸ”¹ Title & Introduction:\n",
      "This scientific paper is titled \"Classification of Sarcoma Based on Genomic Data Using Machine Learning Models.\" The primary objective of this study revolves around developing an effective machine learning-based system capable of accurately classifying sarcoma types using genomic data. The researchers employed a novel approach by stacking multiple base classifiers to enhance the overall performance and accuracy of their model. The background context provided in the introduction section highlights the significance of utilizing machine learning models in medical applications, specifically for cancer classification. The study emphasizes that conventional methods often struggle with high-dimensional genomic data and inconsistent evaluation metrics, leading to difficulties in replicating results. Furthermore, the authors discuss the challenges associated with managing complex medical information, such as patient history, genetic factors, and clinical images. This underscores the need for innovative approaches like machine learning-based systems, which can extract patterns and insights from large datasets, ultimately contributing to early disease detection, personalized treatment plans, and improved patient outcomes. The researchers' approach involves a comprehensive methodology that includes data splitting, base classifier training using techniques such as XGBoost, LightGBM, and Random Forest, and a stacking process that minimizes the loss function. The study also evaluates the performance of various machine learning models, including Support Vector Machine, KNN, Random Forest, Xgboost, and Light GBM, through metrics such as accuracy, precision, and recall. This comprehensive analysis serves as a foundation for future research, enabling the development of more robust machine learning-based systems for cancer classification and genomic data analysis.\n",
      "\n",
      "\n",
      "ðŸ”¹ Key Topics & Sections:\n",
      "This document focuses on the development of a Machine Learning-based system for sarcoma classification, aiming to improve accuracy and objectivity over traditional histopathological examination methods. The study proposes a workflow that involves several key steps, including data splitting, base classifier training, and meta-classifier stacking. The latter process utilizes techniques such as Random Forest and LightGBM to synthesize predictions from individual classifiers, minimizing the loss function through iterative refinement. The main topics covered in this document can be broadly categorized into methodology, evaluation, and discussion. The methodology section describes the workflow and pre-processing steps involved in preparing the dataset for analysis. This includes feature extraction, normalization, and data wrangling techniques to effectively handle both categorical and numerical features. Ensemble learning methods are also employed to combine multiple base models into a stacking classifier, thereby improving accuracy and mitigating issues such as overfitting. The document highlights the potential of Machine Learning models in disrupting traditional approaches to medical diagnosis and treatment planning. By leveraging complex algorithms that can analyze patterns in genomic data, clinical images, and patient history, researchers aim to detect diseases at an early stage and design personalized treatment plans. The study's findings are evaluated through performance metrics such as accuracy, precision, and recall, with results indicating the effectiveness of the proposed system compared to other models. Overall, this research contributes to the development of more accurate and objective approaches to sarcoma classification, which can enhance patient outcomes and save lives.\n",
      "\n",
      "\n",
      "ðŸ”¹ Definitions & Key Terms:\n",
      "various specialized terminology, concepts, and models utilized in machine learning-based classification systems, particularly in the context of genomic data analysis. Key terms such as \"Genomic Classification Machine Learning,\" \"Ensemble Learning Methods,\" and \"Stacking Classifier\" are introduced to describe an advanced technique involving computational technologies for analyzing and interpreting genetic material. Genomic classification is defined as a method that combines the strengths of individual base models by stacking them together, thus minimizing loss functions and improving overall accuracy. Furthermore, the document highlights several technical terms essential to machine learning-based systems, including \"Data Splitting,\" \"Base Classifier Training,\" and \"5-Fold Cross-Validation.\" These processes are crucial in training and evaluating the performance of machine learning models. Additionally, concepts such as \"Undersampling\" and \"Oversampling\" are introduced as techniques used to balance class distributions and optimize model performance. The document also touches on meta-learning, mentioning \"Meta-Level Learning Technique\" and \"Meta-Level Features,\" which aim to improve classification accuracy by employing advanced features. The importance of these specialized terminology, concepts, and models is underscored in the context of complex medical information analysis, where machine learning algorithms can identify patterns and insights that may seem enigmatic to humans. These systems have the potential to disrupt various areas in medicine, enabling early disease detection, personalized treatment plans, and ultimately saving lives. The document concludes by suggesting extensions to the methodology employed, including data cleaning procedures, feature extraction elements, and the design of stacking super classifiers, which can be adapted for similar problems involving genomic data analysis.\n",
      "\n",
      "\n",
      "ðŸ”¹ Methods & Approaches:\n",
      "The study employed various machine learning-based methodologies, including tree-based classifiers and Artificial Neural Networks, which produced noteworthy results in predicting outcomes. The experimental design involved data splitting, with 80% of preprocessed data used for training and the remaining 20% for testing, maintaining the distribution of sarcoma types. Base classifier training was conducted using XGBoost, LightGBM, and Random Forest algorithms, with 5-fold cross-validation to generate out-of-fold predictions. The study also explored ensemble learning methods, where several base models were combined into a stacking classifier to improve accuracy. Techniques such as undersampling, oversampling, and grid search optimization were employed to achieve desired targets. Additionally, the study utilized genetics-based pattern recognition systems, which involve constructing and training models fed with genomic big data to recognize genetic elements like genes, mutations, and variants within DNA sequences. The machine learning approaches used in this study demonstrate their potential to disrupt various areas in medicine by identifying complex patterns and insights that may be challenging for humans to discern. These models can forecast outcomes based on identified patterns, making them suitable for disease detection at the earliest stage, personalized treatment planning, and ultimately saving lives. Furthermore, the use of techniques such as data splitting, base classifier training, and ensemble learning showcases the iterative refinement process employed in this study, highlighting the importance of experimentation and refinement in achieving optimal results.\n",
      "\n",
      "\n",
      "ðŸ”¹ Findings & Results:\n",
      "The primary findings presented in this document pertain to the evaluation of various machine learning models, including Support Vector Machine, KNN, Random Forest, Xgboost, Light GBM, and others. A bar chart (Fig. 3) displays the results of performance evaluation metrics, specifically accuracy, precision, and recall for these models. The outcomes indicate that tree-based classifiers produced the best results, followed by Artificial Neural Networks, while other models yielded subpar performance. The study's methodology employed data cleaning procedures, feature extraction elements, and the design of a stacking super classifier to address cancer classification. Key features were engineered from genomic data, including chromosome numbers, mutation presence in key genes (e.g., TP53, ATRX, RB1), copy number data segmented into discrete categories, and expression values normalized with pathway activation scores calculated based on gene set enrichment analysis. These approaches enabled the development of machine learning models capable of identifying patterns in complex medical information, such as patient history, genetic factors, and clinical images. The proposed work demonstrates the potential of precision medicine powered by genomics in treating sarcoma. Machine learning models can successfully identify patterns that may seem enigmatic to humans, allowing for early disease detection, personalized treatment plans, and more accurate prognosis assessments. The study's findings have significant implications for the development of individualized treatment plans and highlight the promise of incorporating machine learning models into clinical workflows to enhance medical decision-making.\n",
      "\n",
      "\n",
      "ðŸ”¹ Important Statistics & Data:\n",
      "The presented study employs various quantitative metrics to evaluate the performance of machine learning-based models in classifying sarcomas. Notably, the data splitting process involves dividing the preprocessed data into training (80%) and testing (20%) sets, ensuring that the distribution of sarcoma types is maintained. This approach allows for a comprehensive evaluation of the model's accuracy, sensitivity, and specificity. The study also presents various statistical analyses, including gene set enrichment analysis to calculate pathway activation scores based on normalized expression values. Additionally, the classification error is calculated as the proportion of incorrect predictions, which can be determined by taking the complement of accuracy. Furthermore, the results are compared against other available methods using metrics such as accuracy, sensitivity, and specificity. The study demonstrates that machine learning models, specifically Stacking;Random Forest, LightGBM, can effectively classify sarcomas with high accuracy, outperforming conventional statistical techniques. The dataset used in this study consists of 801 samples from four cancer types: lung adenocarcinoma (LUAD), clear cell kidney carcinoma (KIRC), prostate adenocarcinoma (PRAD), and colon adenocarcinoma (COAD). The models' performance is evaluated using metrics such as accuracy, sensitivity, and specificity, with the area under the curve being a key indicator of their effectiveness. The results show that the configuration of these models is particularly useful for genetic data analysis due to its complexity, highlighting the potential of machine learning techniques in improving diagnostic procedures.\n",
      "\n",
      "\n",
      "ðŸ”¹ Applications & Use Cases:\n",
      "The application of machine learning (ML) models in real-world contexts is a significant focus of this study. These models, such as Support Vector Machine, KNN, Random Forest, Xgboost, and Light GBM, have been shown to produce noteworthy results in the classification of sarcoma, outperforming other models. The use of these ML models enables the identification of complex patterns in genetic data that conventional statistical techniques may miss. By leveraging these models, it becomes feasible to detect diseases at an early stage, design personalized treatment plans, and ultimately save lives. The study's findings have practical implications for the diagnosis and treatment of sarcoma. Machine learning-based systems can assess complex medical information, including patient history, genetic factors, and clinical images, to identify patterns and insights that may seem enigmatic to humans. These systems can successfully detect diseases, design effective treatment plans tailored to individual patients, and contribute to improved healthcare outcomes. The potential for machine learning models to disrupt many areas in medicine is also highlighted, with genetics-based pattern recognition systems emerging as a crucial technique in understanding complex biological systems. The methodology employed in this study provides a structure for similar works, enabling the extension of data cleaning procedures, feature extraction elements, and the design of stacking super classifiers. This framework can be adapted to other tasks where cancer classification is involved, offering a valuable resource for researchers seeking to develop more accurate diagnostic procedures. The study's focus on improving automated sarcoma diagnosis underscores its significance in addressing the challenges associated with this complex disease.\n",
      "\n",
      "\n",
      "ðŸ”¹ Challenges & Limitations:\n",
      "The study acknowledges several limitations, constraints, and challenges in managing high-dimensional genomic data and selecting relevant features consistently. These include the risk of models performing well on training data but poorly on unseen data, especially with limited sample sizes. Additionally, inconsistent model building procedures and evaluation metrics across studies hinder reproducibility, making it challenging to advance the field of meta-learning and strategies for combining classifiers. Furthermore, the researchers highlight the difficulties in handling large-scale genomic datasets due to their enormous dimensionality and complexity. This is exacerbated by the aggressiveness and multifariousness of certain genetic mutations, which pose a major problem in clinical oncology. To mitigate these challenges, the study proposes the use of machine learning (ML) models, which can handle complex medical information like patient history, genetic factors, and clinical images to identify patterns and insights that may seem enigmatic to humans. The limitations of current methods are also acknowledged, with the researchers noting that existing strategies for combining classifiers have limitations in achieving improved classification. To address this, they propose extensions employing meta-level learning techniques and a broader array of meta-level features, which they aim to support through experimentation with several datasets. These efforts aim to advance the field of meta-learning and improve the accuracy of classification, ultimately saving more lives by detecting diseases at an early stage and designing personalized treatment plans.\n",
      "\n",
      "\n",
      "ðŸ”¹ Future Scope & Recommendations:\n",
      "Future research in machine learning and meta-learning is crucial to address the existing gaps and limitations in the field. One such gap is the challenge of managing high-dimensional genomic data and selecting relevant features consistently, which warrants further investigation and development. Additionally, there is a need to mitigate the risk of models performing well on training data but poorly on unseen data, especially with limited sample sizes. This issue can be attributed to inconsistent model building procedures and evaluation metrics across studies, hindering reproducibility. To address these gaps, researchers propose investigating the potential benefits of integrating multiple genomic and clinical data sources, which many studies currently neglect. Furthermore, they suggest exploring the effectiveness of meta-level learning techniques, such as the use of MLR (meta-level learning technique) and a broader array of meta-level features. These extensions can potentially lead to improved classification performance compared to existing methods, thus advancing the field of meta-learning and strategies for combining classifiers. The incorporation of machine learning models into clinical workflows is also an area that warrants further exploration. As these models have the potential to disrupt many areas in medicine, researchers aim to demonstrate their promise in precision medicine powered by genomics, particularly in the treatment of sarcoma. By developing optimal models for sarcoma classification and evaluating their performance using extensive testing and validation procedures, researchers can guarantee the correctness and dependability of research findings, ultimately leading to more individualized treatment plans and more accurate prognosis assessments.\n",
      "\n",
      "\n",
      "ðŸ”¹ Conclusion & Summary:\n",
      "The study's main conclusions highlight the significance of incorporating Machine Learning (ML) models into clinical workflows for treating sarcoma, a complex and heterogeneous cancer type. The proposed work demonstrates the potential of precision medicine powered by genomics in improving treatment plans and prognosis assessments. This is achieved through the development of more precise and complex classification systems that take genetic information into account. The study's key findings indicate that tree-based classifiers and Artificial Neural Networks produced the best results, outperforming other models such as Support Vector Machine and KNN. These models can effectively handle high-dimensional genomic data and identify patterns that conventional statistical techniques might miss. Moreover, the stacking process used in this study minimizes the loss function by synthesizing predictions from base classifiers, leading to more accurate classification systems. The study's contributions also emphasize the importance of addressing existing gaps in ML-based systems for sarcoma treatment. These gaps include challenges in managing high-dimensional genomic data and selecting relevant features consistently, as well as the risk of models performing well on training data but poorly on unseen data, especially with limited sample sizes. The proposed work aims to address these challenges and provide a more robust framework for developing precision medicine solutions powered by genomics.\n",
      "\n",
      "\n",
      "âœ… Extracted structured content saved to C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\presentation_content.txt\n",
      "âœ… ROUGE scores saved to C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\rouge_scores.txt\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import chromadb\n",
    "import ollama\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading required NLTK resources...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Function to generate a unique collection name based on the PDF path\n",
    "def get_collection_name(pdf_path):\n",
    "    \"\"\"Generate a unique collection name based on the PDF filename\"\"\"\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    # Create a hash of the filename to ensure unique collection names\n",
    "    filename_hash = hashlib.md5(pdf_filename.encode()).hexdigest()[:10]\n",
    "    return f\"doc_collection_{filename_hash}\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "DB_PATH = \"./chromadb_improved_store\"\n",
    "os.makedirs(DB_PATH, exist_ok=True)\n",
    "chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Improved system prompt with narrative flow instructions\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert academic content analyzer tasked with extracting highly relevant information from scientific documents.\n",
    "For the given query about a specific section, focus ONLY on extracting the most relevant information from the provided document chunks.\n",
    "\n",
    "IMPORTANT GUIDELINES:\n",
    "1. Create a natural narrative flow - avoid question-answer format\n",
    "2. Do NOT use subsection headers with asterisks or other special formatting\n",
    "3. Provide factual content from the document only, no interpretations\n",
    "4. Maintain academic tone and technical accuracy\n",
    "5. Format response as cohesive paragraphs with smooth transitions\n",
    "6. Ignore publication metadata, acknowledgments, and references unless explicitly requested\n",
    "7. If the provided text doesn't contain relevant information for the query, state this clearly\n",
    "\n",
    "Remember that your output will be used to create a presentation slide, so prioritize clarity, narrative flow, and relevance.\n",
    "\"\"\"\n",
    "\n",
    "# More detailed and specific queries for better extraction\n",
    "PRESENTATION_QUERIES = {\n",
    "    \"Title & Introduction\": \"Extract the title of this scientific paper, main research question/objective, and background context provided in the introduction section.\",\n",
    "    \n",
    "    \"Key Topics & Sections\": \"What are the main topics covered in this document? Identify and describe each major section. Focus on the core subject areas and main themes.\",\n",
    "    \n",
    "    \"Definitions & Key Terms\": \"What specialized terminology, concepts, or models are defined in this document? Extract the important technical terms and their definitions.\",\n",
    "    \n",
    "    \"Methods & Approaches\": \"What specific methodologies, techniques, algorithms or research approaches are described? Include details about experimental design, data collection, and analytical methods.\",\n",
    "    \n",
    "    \"Findings & Results\": \"What are the primary results or findings presented in this document? Include specific outcomes, measurements, statistical results, and key discoveries.\",\n",
    "    \n",
    "    \"Important Statistics & Data\": \"What quantitative data, metrics, percentages, or statistical analyses are presented? Extract specific numbers, measurements, and their context.\",\n",
    "    \n",
    "    \"Applications & Use Cases\": \"How are the findings or methods applied in real-world contexts? What practical applications or implementations are discussed?\",\n",
    "    \n",
    "    \"Challenges & Limitations\": \"What limitations, constraints, or challenges are acknowledged in the research? What are the identified weaknesses or areas for improvement?\",\n",
    "    \n",
    "    \"Future Scope & Recommendations\": \"What suggestions for future research are mentioned? What recommendations or next steps are proposed?\",\n",
    "    \n",
    "    \"Conclusion & Summary\": \"What are the main conclusions or key takeaways? How does the document summarize its contributions and significance?\"\n",
    "}\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove page numbers and headers/footers (patterns like \"Page X of Y\")\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "    \n",
    "    # Remove reference notations like [1], [2,3], etc.\n",
    "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text)\n",
    "    \n",
    "    # Remove common PDF artifacts\n",
    "    text = re.sub(r'https?://\\S+', '', text)  # URLs\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to chunk text into semantic units\n",
    "def chunk_text(text, max_chunk_size=500, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks of roughly equal size while respecting sentence boundaries.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Split into sentences (simple approach)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # If adding this sentence would exceed chunk size and we already have content\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # Keep some overlap for context\n",
    "            overlap_point = max(0, len(current_chunk) - overlap)\n",
    "            current_chunk = current_chunk[overlap_point:] + sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function to extract and store text from PDF with improved chunking\n",
    "# Function to extract and store text from PDF with improved chunking\n",
    "def store_pdf_in_chromadb(pdf_path):\n",
    "    \"\"\"Extract text from PDF and store in a document-specific ChromaDB collection\"\"\"\n",
    "    print(f\"Processing PDF: {pdf_path}\")\n",
    "    \n",
    "    # Generate a unique collection name based on the PDF filename\n",
    "    collection_name = get_collection_name(pdf_path)\n",
    "    \n",
    "    # Try to get existing collection or create new one\n",
    "    try:\n",
    "        # Check if collection exists\n",
    "        existing_collections = chroma_client.list_collections()\n",
    "        collection_exists = any(c.name == collection_name for c in existing_collections)\n",
    "        \n",
    "        if collection_exists:\n",
    "            print(f\"Collection {collection_name} already exists. Will use existing collection.\")\n",
    "            collection = chroma_client.get_collection(name=collection_name)\n",
    "            \n",
    "            # Extract full text for ROUGE evaluation (we still need this)\n",
    "            full_text = \"\"\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        full_text += \" \" + text\n",
    "            full_text = clean_text(full_text)\n",
    "            \n",
    "            return collection_name, full_text\n",
    "        else:\n",
    "            # Create new collection\n",
    "            collection = chroma_client.create_collection(name=collection_name)\n",
    "            print(f\"Created new collection: {collection_name}\")\n",
    "            \n",
    "            # Extract full text from PDF\n",
    "            full_text = \"\"\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        full_text += \" \" + text\n",
    "            \n",
    "            # Clean the full text\n",
    "            full_text = clean_text(full_text)\n",
    "            \n",
    "            # Chunk the text into semantic units\n",
    "            chunks = chunk_text(full_text)\n",
    "            print(f\"Created {len(chunks)} semantic chunks from document\")\n",
    "            \n",
    "            # Store chunks in ChromaDB with meaningful ids\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                collection.add(\n",
    "                    documents=[chunk],\n",
    "                    metadatas=[{\"chunk_id\": i, \"source\": os.path.basename(pdf_path)}],\n",
    "                    ids=[f\"chunk_{i}\"]\n",
    "                )\n",
    "            \n",
    "            print(f\"Successfully stored {len(chunks)} chunks in ChromaDB collection '{collection_name}'\")\n",
    "            return collection_name, full_text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error handling collection: {e}\")\n",
    "        \n",
    "        # As a fallback, try a different collection name with a timestamp\n",
    "        fallback_name = f\"{collection_name}_{int(time.time())}\"\n",
    "        print(f\"Attempting fallback with collection name: {fallback_name}\")\n",
    "        \n",
    "        try:\n",
    "            collection = chroma_client.create_collection(name=fallback_name)\n",
    "            \n",
    "            # Extract full text from PDF\n",
    "            full_text = \"\"\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        full_text += \" \" + text\n",
    "            \n",
    "            # Clean the full text\n",
    "            full_text = clean_text(full_text)\n",
    "            \n",
    "            # Chunk the text into semantic units\n",
    "            chunks = chunk_text(full_text)\n",
    "            \n",
    "            # Store chunks in ChromaDB with meaningful ids\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                collection.add(\n",
    "                    documents=[chunk],\n",
    "                    metadatas=[{\"chunk_id\": i, \"source\": os.path.basename(pdf_path)}],\n",
    "                    ids=[f\"chunk_{i}\"]\n",
    "                )\n",
    "            \n",
    "            return fallback_name, full_text\n",
    "        except Exception as e2:\n",
    "            print(f\"Fatal error with ChromaDB: {e2}\")\n",
    "            raise\n",
    "\n",
    "# ROUGE Score Implementation\n",
    "def calculate_rouge_scores(reference_text, generated_text):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-N scores for n=1, n=2, and ROUGE-L score.\n",
    "    \n",
    "    Args:\n",
    "        reference_text (str): The original text\n",
    "        generated_text (str): The generated text to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    # Ensure NLTK punkt tokenizer is available\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Tokenize texts\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    generated_tokens = nltk.word_tokenize(generated_text.lower())\n",
    "    \n",
    "    # ROUGE-1 (unigrams)\n",
    "    reference_unigrams = Counter(reference_tokens)\n",
    "    generated_unigrams = Counter(generated_tokens)\n",
    "    common_unigrams = reference_unigrams & generated_unigrams\n",
    "    \n",
    "    rouge_1_precision = sum(common_unigrams.values()) / max(1, sum(generated_unigrams.values()))\n",
    "    rouge_1_recall = sum(common_unigrams.values()) / max(1, sum(reference_unigrams.values()))\n",
    "    rouge_1_f1 = 2 * rouge_1_precision * rouge_1_recall / max(1e-10, rouge_1_precision + rouge_1_recall)\n",
    "    \n",
    "    # ROUGE-2 (bigrams)\n",
    "    reference_bigrams = Counter(list(ngrams(reference_tokens, 2)))\n",
    "    generated_bigrams = Counter(list(ngrams(generated_tokens, 2)))\n",
    "    common_bigrams = reference_bigrams & generated_bigrams\n",
    "    \n",
    "    rouge_2_precision = sum(common_bigrams.values()) / max(1, sum(generated_bigrams.values()))\n",
    "    rouge_2_recall = sum(common_bigrams.values()) / max(1, sum(reference_bigrams.values()))\n",
    "    rouge_2_f1 = 2 * rouge_2_precision * rouge_2_recall / max(1e-10, rouge_2_precision + rouge_2_recall)\n",
    "    \n",
    "    # ROUGE-L (Longest Common Subsequence)\n",
    "    lcs_length = compute_lcs_length(reference_tokens, generated_tokens)\n",
    "    \n",
    "    rouge_l_precision = lcs_length / max(1, len(generated_tokens))\n",
    "    rouge_l_recall = lcs_length / max(1, len(reference_tokens))\n",
    "    rouge_l_f1 = 2 * rouge_l_precision * rouge_l_recall / max(1e-10, rouge_l_precision + rouge_l_recall)\n",
    "    \n",
    "    return {\n",
    "        \"ROUGE-1\": {\n",
    "            \"precision\": rouge_1_precision,\n",
    "            \"recall\": rouge_1_recall,\n",
    "            \"f1\": rouge_1_f1\n",
    "        },\n",
    "        \"ROUGE-2\": {\n",
    "            \"precision\": rouge_2_precision,\n",
    "            \"recall\": rouge_2_recall,\n",
    "            \"f1\": rouge_2_f1\n",
    "        },\n",
    "        \"ROUGE-L\": {\n",
    "            \"precision\": rouge_l_precision,\n",
    "            \"recall\": rouge_l_recall,\n",
    "            \"f1\": rouge_l_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "def compute_lcs_length(X, Y):\n",
    "    \"\"\"Compute the length of the Longest Common Subsequence (LCS) between two token lists\"\"\"\n",
    "    m, n = len(X), len(Y)\n",
    "    \n",
    "    # Create LCS matrix\n",
    "    L = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                L[i][j] = 0\n",
    "            elif X[i-1] == Y[j-1]:\n",
    "                L[i][j] = L[i-1][j-1] + 1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    "    \n",
    "    return L[m][n]\n",
    "\n",
    "# Function to find relevant chunks for evaluation\n",
    "def get_relevant_chunks_for_query(query, full_text, window_size=3000):\n",
    "    \"\"\"\n",
    "    Find the most relevant chunk of text from the PDF for the given query.\n",
    "    This provides context for ROUGE evaluation.\n",
    "    \"\"\"\n",
    "    # Ensure NLTK punkt tokenizer is available\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Tokenize the full text\n",
    "    sentences = nltk.sent_tokenize(full_text)\n",
    "    \n",
    "    # Create sliding windows of sentences to find the most relevant section\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), window_size // 50):  # Approximate words per sentence\n",
    "        end_idx = min(i + window_size // 50, len(sentences))\n",
    "        chunk = \" \".join(sentences[i:end_idx])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Basic keyword matching to find most relevant chunk (simplified approach)\n",
    "    query_keywords = set(nltk.word_tokenize(query.lower()))\n",
    "    \n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_tokens = set(nltk.word_tokenize(chunk.lower()))\n",
    "        overlap = len(query_keywords.intersection(chunk_tokens))\n",
    "        score = overlap / max(1, len(query_keywords))\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    \n",
    "    # If no good match found, return the first chunk\n",
    "    if best_score < 0.1 and chunks:\n",
    "        return chunks[0]\n",
    "    \n",
    "    return best_chunk\n",
    "\n",
    "# Function to retrieve relevant content using keywords\n",
    "def retrieve_relevant_chunks(query, collection_name, n_results=10):\n",
    "    \"\"\"Retrieve the most relevant chunks for a given query from a specific collection.\"\"\"\n",
    "    try:\n",
    "        # Get the specific collection for this PDF\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "        \n",
    "        results = collection.query(\n",
    "            query_texts=[query], \n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        if results[\"documents\"] and len(results[\"documents\"][0]) > 0:\n",
    "            # Return all retrieved chunks\n",
    "            return results[\"documents\"][0]\n",
    "        else:\n",
    "            return [\"No relevant content found.\"]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving chunks: {e}\")\n",
    "        return [\"Error retrieving content.\"]\n",
    "\n",
    "# Enhance query with specific focus areas\n",
    "def enhance_query(section, query):\n",
    "    \"\"\"Add context to the query based on the section.\"\"\"\n",
    "    enhancements = {\n",
    "        \"Title & Introduction\": \"Provide a natural flowing paragraph about the document title, research questions, and introductory context. Do not use any subsection headers or asterisks.\",\n",
    "        \"Findings & Results\": \"Extract specific outcomes, measurements, and discoveries in a narrative format. Include key statistics if available. Avoid using subsection headers or question-answer format.\",\n",
    "        \"Methods & Approaches\": \"Describe the specific techniques, algorithms, experiments, or methodological approaches in flowing paragraphs. Do not use subsection markers or bullet points.\",\n",
    "    }\n",
    "    \n",
    "    if section in enhancements:\n",
    "        return f\"{query}\\n\\nAdditional guidance: {enhancements[section]}\"\n",
    "    return query\n",
    "\n",
    "# Function to generate naturally flowing sections with improved prompting\n",
    "def generate_presentation_sections(pdf_path):\n",
    "    structured_presentation = {}\n",
    "    rouge_scores = {}  # To store ROUGE scores for each section\n",
    "    \n",
    "    # Process PDF and get collection name specific to this PDF\n",
    "    collection_name, full_text = store_pdf_in_chromadb(pdf_path)\n",
    "    \n",
    "    # Generate each section\n",
    "    for section, base_query in PRESENTATION_QUERIES.items():\n",
    "        print(f\"Processing section: {section}\")\n",
    "        \n",
    "        # Enhance the query with section-specific guidance\n",
    "        enhanced_query = enhance_query(section, base_query)\n",
    "        \n",
    "        # Retrieve relevant content from the PDF-specific collection\n",
    "        retrieved_chunks = retrieve_relevant_chunks(enhanced_query, collection_name)\n",
    "        \n",
    "        # Skip if no relevant content\n",
    "        if not retrieved_chunks or all(chunk == \"No relevant content found.\" for chunk in retrieved_chunks):\n",
    "            structured_presentation[section] = \"No relevant information found in the document.\"\n",
    "            rouge_scores[section] = {\n",
    "                \"ROUGE-1\": {\"f1\": 0.0},\n",
    "                \"ROUGE-2\": {\"f1\": 0.0},\n",
    "                \"ROUGE-L\": {\"f1\": 0.0}\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Combine chunks for context but limit total size\n",
    "        combined_text = \" \".join(retrieved_chunks)\n",
    "        if len(combined_text) > 4000:  # Limit context size for LLM\n",
    "            combined_text = combined_text[:4000] + \"...\"\n",
    "        \n",
    "        # Craft detailed prompt for each section with narrative flow instructions\n",
    "        prompt = f\"\"\"\n",
    "        I need to extract content for the \"{section}\" section of a presentation based on a scientific document.\n",
    "\n",
    "        Query: {base_query}\n",
    "        \n",
    "        Document content:\n",
    "        {combined_text}\n",
    "        \n",
    "        Create 2-3 naturally flowing paragraphs that address the query. IMPORTANT:\n",
    "        1. DO NOT use subsection headers with asterisks (like **Title** or **Main Research Question**)\n",
    "        2. DO NOT format as question-answer pairs\n",
    "        3. Create smooth transitions between ideas\n",
    "        4. Maintain an academic but narrative tone\n",
    "        5. Only include information from the document\n",
    "        6. MAXIMIZE the use of KEY TERMINOLOGY from the original text\n",
    "        7. Preserve as many of the original phrases and technical terms as possible\n",
    "        8. Ensure comprehensive coverage of all main points relevant to the query\n",
    "        \n",
    "        The output should read like a coherent mini-essay suitable for a presentation slide.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Throttle API calls to prevent rate limiting\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Get response from LLM\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=\"llama3.1\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )[\"message\"][\"content\"]\n",
    "            \n",
    "            # Clean and format the response\n",
    "            cleaned_response = clean_text(response)\n",
    "            \n",
    "            # Remove any \"based on the document\" phrases and LLM disclaimers\n",
    "            cleaned_response = re.sub(r'Based on the (provided|document|given|available) (content|text|information|document)', '', cleaned_response)\n",
    "            cleaned_response = re.sub(r'From the (provided|document|given|available) (content|text|information|document)', '', cleaned_response)\n",
    "            cleaned_response = re.sub(r'According to the (provided|document|given|available) (content|text|information|document)', '', cleaned_response)\n",
    "            cleaned_response = re.sub(r'The document (states|mentions|indicates|suggests|notes|describes|discusses|presents|shows|reports|provides|explains)', '', cleaned_response)\n",
    "            \n",
    "            # Remove any model disclaimers\n",
    "            cleaned_response = re.sub(r'I don\\'t have enough information to.*', '', cleaned_response)\n",
    "            cleaned_response = re.sub(r'The text doesn\\'t (specify|mention|provide|include).*', '', cleaned_response)\n",
    "            \n",
    "            # Remove any remaining double asterisks subsection markers\n",
    "            cleaned_response = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', cleaned_response)\n",
    "            \n",
    "            # Final cleanup and store\n",
    "            structured_presentation[section] = cleaned_response.strip()\n",
    "            \n",
    "            # Find the most relevant section in the original text for ROUGE evaluation\n",
    "            relevant_reference_text = get_relevant_chunks_for_query(base_query, full_text)\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            section_rouge_scores = calculate_rouge_scores(relevant_reference_text, cleaned_response)\n",
    "            rouge_scores[section] = section_rouge_scores\n",
    "            \n",
    "            # Print ROUGE scores for this section\n",
    "            print(f\"  ROUGE scores for '{section}':\")\n",
    "            print(f\"    ROUGE-1 F1: {section_rouge_scores['ROUGE-1']['f1']:.4f}\")\n",
    "            print(f\"    ROUGE-2 F1: {section_rouge_scores['ROUGE-2']['f1']:.4f}\")\n",
    "            print(f\"    ROUGE-L F1: {section_rouge_scores['ROUGE-L']['f1']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating content for '{section}': {e}\")\n",
    "            structured_presentation[section] = f\"Error extracting content for this section: {str(e)}\"\n",
    "            rouge_scores[section] = {\n",
    "                \"ROUGE-1\": {\"f1\": 0.0},\n",
    "                \"ROUGE-2\": {\"f1\": 0.0},\n",
    "                \"ROUGE-L\": {\"f1\": 0.0}\n",
    "            }\n",
    "    \n",
    "    return structured_presentation, rouge_scores\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Define PDF path\n",
    "    pdf_path = r'C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Sarcoma.pdf'\n",
    "    \n",
    "    # Generate structured presentation content with ROUGE evaluation\n",
    "    presentation_content, rouge_scores = generate_presentation_sections(pdf_path)\n",
    "    \n",
    "    # Calculate average ROUGE scores across all sections\n",
    "    avg_rouge_1 = sum(score[\"ROUGE-1\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "    avg_rouge_2 = sum(score[\"ROUGE-2\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "    avg_rouge_l = sum(score[\"ROUGE-L\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "    \n",
    "    print(\"\\n===== ROUGE Score Summary =====\")\n",
    "    print(f\"Average ROUGE-1 F1: {avg_rouge_1:.4f}\")\n",
    "    print(f\"Average ROUGE-2 F1: {avg_rouge_2:.4f}\")\n",
    "    print(f\"Average ROUGE-L F1: {avg_rouge_l:.4f}\")\n",
    "    print(\"===============================\")\n",
    "    \n",
    "    # Save structured content and ROUGE scores\n",
    "    output_file = r\"C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\presentation_content.txt\"\n",
    "    rouge_output_file = r\"C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\rouge_scores.txt\"\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for section, content in presentation_content.items():\n",
    "            print(f\"\\nðŸ”¹ {section}:\\n{content}\\n\")\n",
    "            f.write(f\"{section}:\\n{content}\\n\\n\")\n",
    "    \n",
    "    with open(rouge_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"===== ROUGE Score Summary =====\\n\")\n",
    "        f.write(f\"Average ROUGE-1 F1: {avg_rouge_1:.4f}\\n\")\n",
    "        f.write(f\"Average ROUGE-2 F1: {avg_rouge_2:.4f}\\n\")\n",
    "        f.write(f\"Average ROUGE-L F1: {avg_rouge_l:.4f}\\n\")\n",
    "        f.write(\"===============================\\n\\n\")\n",
    "        \n",
    "        for section, scores in rouge_scores.items():\n",
    "            f.write(f\"{section}:\\n\")\n",
    "            f.write(f\"  ROUGE-1: P={scores['ROUGE-1']['precision']:.4f}, R={scores['ROUGE-1']['recall']:.4f}, F1={scores['ROUGE-1']['f1']:.4f}\\n\")\n",
    "            f.write(f\"  ROUGE-2: P={scores['ROUGE-2']['precision']:.4f}, R={scores['ROUGE-2']['recall']:.4f}, F1={scores['ROUGE-2']['f1']:.4f}\\n\")\n",
    "            f.write(f\"  ROUGE-L: P={scores['ROUGE-L']['precision']:.4f}, R={scores['ROUGE-L']['recall']:.4f}, F1={scores['ROUGE-L']['f1']:.4f}\\n\\n\")\n",
    "    \n",
    "    print(f\"\\nâœ… Extracted structured content saved to {output_file}\")\n",
    "    print(f\"âœ… ROUGE scores saved to {rouge_output_file}\")\n",
    "\n",
    "# Support processing multiple PDFs\n",
    "def process_multiple_pdfs(pdf_paths):\n",
    "    \"\"\"Process multiple PDFs and save individual results\"\"\"\n",
    "    all_rouge_scores = {}\n",
    "    \n",
    "    for pdf_path in pdf_paths:\n",
    "        # Create output filename based on PDF name\n",
    "        pdf_basename = os.path.basename(pdf_path).split('.')[0]\n",
    "        output_file = f\"{os.path.dirname(pdf_path)}/{pdf_basename}_presentation.txt\"\n",
    "        rouge_output_file = f\"{os.path.dirname(pdf_path)}/{pdf_basename}_rouge_scores.txt\"\n",
    "        \n",
    "        print(f\"\\nðŸ“„ Processing PDF: {pdf_path}\")\n",
    "        \n",
    "        # Generate content for this specific PDF with ROUGE evaluation\n",
    "        presentation_content, rouge_scores = generate_presentation_sections(pdf_path)\n",
    "        all_rouge_scores[pdf_basename] = rouge_scores\n",
    "        \n",
    "        # Calculate average ROUGE scores for this PDF\n",
    "        avg_rouge_1 = sum(score[\"ROUGE-1\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "        avg_rouge_2 = sum(score[\"ROUGE-2\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "        avg_rouge_l = sum(score[\"ROUGE-L\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "        \n",
    "        print(f\"\\n===== ROUGE Score Summary for {pdf_basename} =====\")\n",
    "        print(f\"Average ROUGE-1 F1: {avg_rouge_1:.4f}\")\n",
    "        print(f\"Average ROUGE-2 F1: {avg_rouge_2:.4f}\")\n",
    "        print(f\"Average ROUGE-L F1: {avg_rouge_l:.4f}\")\n",
    "        \n",
    "        # Save structured content\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for section, content in presentation_content.items():\n",
    "                print(f\"\\nðŸ”¹ {section}:\\n{content}\\n\")\n",
    "                f.write(f\"{section}:\\n{content}\\n\\n\")\n",
    "        \n",
    "        # Save ROUGE scores\n",
    "        with open(rouge_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"===== ROUGE Score Summary for {pdf_basename} =====\\n\")\n",
    "            f.write(f\"Average ROUGE-1 F1: {avg_rouge_1:.4f}\\n\")\n",
    "            f.write(f\"Average ROUGE-2 F1: {avg_rouge_2:.4f}\\n\")\n",
    "            f.write(f\"Average ROUGE-L F1: {avg_rouge_l:.4f}\\n\")\n",
    "            f.write(\"===============================\\n\\n\")\n",
    "            \n",
    "            for section, scores in rouge_scores.items():\n",
    "                f.write(f\"{section}:\\n\")\n",
    "                f.write(f\"  ROUGE-1: P={scores['ROUGE-1']['precision']:.4f}, R={scores['ROUGE-1']['recall']:.4f}, F1={scores['ROUGE-1']['f1']:.4f}\\n\")\n",
    "                f.write(f\"  ROUGE-2: P={scores['ROUGE-2']['precision']:.4f}, R={scores['ROUGE-2']['recall']:.4f}, F1={scores['ROUGE-2']['f1']:.4f}\\n\")\n",
    "                f.write(f\"  ROUGE-L: P={scores['ROUGE-L']['precision']:.4f}, R={scores['ROUGE-L']['recall']:.4f}, F1={scores['ROUGE-L']['f1']:.4f}\\n\\n\")\n",
    "        \n",
    "        print(f\"\\nâœ… Extracted content saved to {output_file}\")\n",
    "        print(f\"âœ… ROUGE scores saved to {rouge_output_file}\")\n",
    "    \n",
    "    # Generate comparative report for multiple PDFs\n",
    "    if len(pdf_paths) > 1:\n",
    "        comparative_report_file = f\"{os.path.dirname(pdf_paths[0])}/comparative_rouge_report.txt\"\n",
    "        \n",
    "        with open(comparative_report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"===== Comparative ROUGE Score Report =====\\n\\n\")\n",
    "            \n",
    "            # Calculate and write overall averages\n",
    "            for pdf_basename, rouge_scores in all_rouge_scores.items():\n",
    "                avg_rouge_1 = sum(score[\"ROUGE-1\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "                avg_rouge_2 = sum(score[\"ROUGE-2\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "                avg_rouge_l = sum(score[\"ROUGE-L\"][\"f1\"] for score in rouge_scores.values()) / len(rouge_scores)\n",
    "                \n",
    "                f.write(f\"Document: {pdf_basename}\\n\")\n",
    "                f.write(f\"  Average ROUGE-1 F1: {avg_rouge_1:.4f}\\n\")\n",
    "                f.write(f\"  Average ROUGE-2 F1: {avg_rouge_2:.4f}\\n\")\n",
    "                f.write(f\"  Average ROUGE-L F1: {avg_rouge_l:.4f}\\n\\n\")\n",
    "        \n",
    "        print(f\"\\nâœ… Comparative ROUGE report saved to {comparative_report_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    # Example of processing multiple PDFs:\n",
    "    # pdf_paths = [\n",
    "    #     r'C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Stock_Market_Prediction.pdf',\n",
    "    #     r'C:\\Users\\pandl\\OneDrive\\Desktop\\FYP\\Climate_Change_Research.pdf'\n",
    "    # ]\n",
    "    # process_multiple_pdfs(pdf_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
